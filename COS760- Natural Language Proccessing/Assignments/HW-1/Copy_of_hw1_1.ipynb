{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4XyILiJcjt-"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tUE9v_Cqcjt_"
      },
      "outputs": [],
      "source": [
        "NAME = \"Kamogelo Motlhale\"\n",
        "StudentID = \"u25743971\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aXol6YscjuB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "lm_nN1m4cjuB",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2ca3080e6e017ba2614b9a4fbd092d2a",
          "grade": false,
          "grade_id": "jupyter",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# COS760 2025 - HW1-1 (20 points)\n",
        "\n",
        "For this problem set, we'll be using the Jupyter notebook:\n",
        "\n",
        "![](jupyter.png)\n",
        "\n",
        "\n",
        "Author(s): Prof Vukosi Marivate, Fiskani Banda\n",
        "\n",
        "**Note:** *This notebooks is only for students taking COS 760. It should not be shared outside the class without the permission of the lecturer(s).*\n",
        "\n",
        "\n",
        "\n",
        "**NLP Fundamentals and Text Preprocessing â€“ Homework Assignment**  \n",
        "\n",
        "Welcome to your first **NLP homework**! This assignment will test your understanding of **Natural Language Processing (NLP)** concepts and challenge you with practical text preprocessing tasks.  \n",
        "\n",
        "- Apply text preprocessing techniques using **NLTK and regular expressions** to clean and analyze text. You'll convert text to lowercase, remove punctuation, and filter out stopwordsâ€”key steps in real-world NLP tasks.  \n",
        "- ðŸ” **Bonus Challenge:** Compare NLTKâ€™s approach with sklearnâ€™s preprocessingâ€”spot the differences and refine your understanding of text cleaning!  \n",
        "\n",
        "By completing this assignment, youâ€™ll **build essential NLP skills** that will help you tackle real-world language challenges. Ready to dive in? ðŸš€\n",
        "\n",
        "#### Resources\n",
        "\n",
        "You might want to get acquianted with the online free edition of the great book \"Natural Language Processing with Python\" [NLTK Book](https://www.nltk.org/book/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIYQD1zgcjuB"
      },
      "source": [
        "---\n",
        "## Get Comfortable with Google Colab\n",
        "\n",
        "Execute a simple Python cell in Colab.\n",
        "\n",
        "**ðŸ“Œ Instruction:** Execute a simple Python cell in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "6vYD-epfcjuC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dcbca7fb4260bdb3f95aad46056b5668",
          "grade": false,
          "grade_id": "cell-d318f217bbc3b6f2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "bbe0fe61-df18-45b9-82e4-fb642e220314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, COS760 NLP!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "print(\"Hello, COS760 NLP!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-ExpMzGlcjuC",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "261332e73ae339ae6ad922c3b35bd79b",
          "grade": false,
          "grade_id": "cell-08af54f720b9f684",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**ðŸ“Œ Instruction:** Run shell commands in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "wdned-IPcjuC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c159acb4840564eba91e8269914ebff1",
          "grade": false,
          "grade_id": "cell-f7fe14248937d9cf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "2876746b-387e-4071-b014-3577975553c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m460.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m556.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m681.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m35.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install nltk #Install nltk in current environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.25.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "Successfully installed scikit-learn-1.6.1 threadpoolctl-3.5.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install scikit-learn #Install sklearn in current python env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tts2vyTMcjuC",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6a87b5c92d957314efa42ed2296966bb",
          "grade": false,
          "grade_id": "cell-f682d5611a96bb67",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "---\n",
        "## Main Assignment\n",
        "\n",
        "Using Python, split the following text into words and sentences manually and using nltk.tokenize. Compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzcyQwDscjuD",
        "outputId": "2977a230-a4f7-42e3-81da-b541554ce720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kamogelomotlhale/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/kamogelomotlhale/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Prof. Marivate and Dr. Modupe teach NLP at UP. The University of Pretoria (UP) is in South Africa. NLP is challenging! However, many researchers and companies are working to solve its problems.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNQV7GHIcjuD"
      },
      "source": [
        "1. Tokenize the given text using the function**[nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html)** and save the tokenized text in variable`nltk_tokens`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Va50jYbRQHz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "0X2pVwhdcjuD",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "acd7172ac94bef4317e9e9d82e8b9f4c",
          "grade": false,
          "grade_id": "cell-f16aa844a5d4b143",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "038b62b7-ba57-4e54-8cef-f918eb4c4546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Tokens: ['Prof.', 'Marivate', 'and', 'Dr.', 'Modupe', 'teach', 'NLP', 'at', 'UP', '.', 'The', 'University', 'of', 'Pretoria', '(', 'UP', ')', 'is', 'in', 'South', 'Africa', '.', 'NLP', 'is', 'challenging', '!', 'However', ',', 'many', 'researchers', 'and', 'companies', 'are', 'working', 'to', 'solve', 'its', 'problems', '.']\n"
          ]
        }
      ],
      "source": [
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "print(\"NLTK Tokens:\", nltk_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_4FBIXJucjuD",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9f472fc977491baadddfa14d445e74b8",
          "grade": true,
          "grade_id": "cell-b57a35a80a11a4ed",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert isinstance(nltk_tokens, list), \"NLTK tokenization failed.\" #1 point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rlWlNDHpcjuE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2333e42340bd013bde6a1e97a95dc086",
          "grade": true,
          "grade_id": "cell-cb078cc15b92cda7",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert len(nltk_tokens) == 39, \"NLTK token list is empty.\" # 2 point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgRynhancjuE"
      },
      "source": [
        "2. Tokenize the given text using **[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)** and **[CountVectorizer.get_feature_names_out()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out)** in variable`sklearn_tokens`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "id": "aZQcr44JcjuE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ae7defa170d3349aa403bb5c74d65e5",
          "grade": false,
          "grade_id": "cell-114b1969a640cd58",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scikit-learn Tokens: ['africa', 'and', 'are', 'at', 'challenging', 'companies', 'dr', 'however', 'in', 'is', 'its', 'many', 'marivate', 'modupe', 'nlp', 'of', 'pretoria', 'problems', 'prof', 'researchers', 'solve', 'south', 'teach', 'the', 'to', 'university', 'up', 'working']\n"
          ]
        }
      ],
      "source": [
        "doc_lst = [] #declare an empty document list\n",
        "doc_lst.append(text) #append the given text to the documents list\n",
        "vector = CountVectorizer() #initiate a count vector object\n",
        "vector.fit_transform(doc_lst) #learn the vocabulary dictionary and return document-term matrix.\n",
        "sklearn_tokens = vector.get_feature_names_out().tolist() #Get output feature names for transformation.\n",
        "\n",
        "print(\"Scikit-learn Tokens:\", sklearn_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "L9Oq-bGDcjuE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d3c5d2a68970d94f992e96ee2ead900e",
          "grade": true,
          "grade_id": "cell-91ef45333f6f10b7",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert isinstance(sklearn_tokens, list), \"Scikit-learn tokenization failed.\" # 1 point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "G72-zjSXcjuE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7170b59b81b3b1e593e3fcddc99b609e",
          "grade": true,
          "grade_id": "cell-d60ffbe26d0924e8",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert len(sklearn_tokens) == 28, \"Scikit-learn token list is empty.\" # 4 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_hiVhjGtcjuE",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "598ba4fecf73290cb549ffa3ef96e3db",
          "grade": false,
          "grade_id": "cell-301c7ba231db2633",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "3. **Question (Markdown response)** [3 points]:  \n",
        "* Compare NLTK vs. Scikit-learn tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.NLTK is able to handle contractions well not just spliting text bases on open spcaces and puntuation, while the Scikit-learn just splits the sentance based on white spaces or punctuation\n",
        "2. NLTK requires you to just call the word_tokenize() function and only provide the text, while Scikit-learn requires a number of preproccessing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "0AOiYVaKcjuE",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cbc244720490476d273f581684a9cc5c",
          "grade": true,
          "grade_id": "cell-f82467dc0b20273b",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "### Answer 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EM0v0tcjuE"
      },
      "source": [
        "4. Convert text to **lowercase**, **remove punctuation** (use regular expression with `re`), and remove **stopwords** using `NLTK`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "deletable": false,
        "id": "jg_csBOBcjuF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2fde0dfa6f34e71f92bf28ec68cfccc5",
          "grade": false,
          "grade_id": "cell-6527db2cbde486ea",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Cleaned: ['prof', 'marivate', 'dr', 'modupe', 'teach', 'nlp', 'university', 'pretoria', 'south', 'africa', 'nlp', 'challenging', 'however', 'many', 'researchers', 'companies', 'working', 'solve', 'problems']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kamogelomotlhale/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "nltk_cleaned = text.lower()\n",
        "nltk_cleaned = re.sub(r'[^\\w\\s]', '', nltk_cleaned)\n",
        "nltk_cleaned = nltk.word_tokenize(nltk_cleaned)\n",
        "\n",
        "nltk_cleaned = [word for word in nltk_cleaned if word not in stop_words]\n",
        "\n",
        "\n",
        "print(\"NLTK Cleaned:\", nltk_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RTJv67h4cjuF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dcf155fc688633c5ca8c9047fe127f81",
          "grade": true,
          "grade_id": "cell-feb0c3923a0ab884",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert isinstance(nltk_cleaned, list), \"NLTK tokenization failed.\" # 1 point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XiKhqII-cjuF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d9f9968702e3e08af38ac57e586a2092",
          "grade": true,
          "grade_id": "cell-1bb270dc5bd12cd2",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert len(nltk_cleaned) == 19, \"NLTK token list is empty.\" # 4 point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7cV0wS-cjuF"
      },
      "source": [
        "5. **Question (Markdown response)** (4 points):  \n",
        "- Contrast the NLTK cleaned result with the sklearn one from question 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "id": "DIOxX1EacjuF",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "07552f93e97624bf49813fdcd551930b",
          "grade": true,
          "grade_id": "cell-784db958fe07bf68",
          "locked": false,
          "points": 4,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "Scikit-learn Tokens returned a longer list of 28 and nltk_cleaned returned only 19 stop_words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
